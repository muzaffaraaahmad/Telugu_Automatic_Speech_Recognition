{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a2b046-13c7-42a5-b365-8757f612954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available!\n",
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ GPU is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ GPU is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2063e3-8fae-4b0f-9cef-90ba7625f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install wandb\n",
    "# !pip install --upgrade pip\n",
    "# !pip install jiwer\n",
    "# !pip install evaluate\n",
    "# !pip install tensorboard\n",
    "# !pip install datasets\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade torch\n",
    "# !pip install --upgrade torchvision\n",
    "# !pip install --upgrade torchaudio\n",
    "# !pip install librosa\n",
    "# !pip install numpy==2.1.0\n",
    "# !pip install scipy==1.11.4\n",
    "# !pip install librosa==0.10.1\n",
    "# !pip install numba==0.58.1\n",
    "# !pip install datasets>=2.14.0\n",
    "# !pip install accelerate>=0.26.0\n",
    "# !pip install typing_extensions --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1232436-f697-416b-9088-3017691d4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install --upgrade torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb1e23d-e6b4-423d-88a0-09c7c0424ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub --quiet\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb4912f-7663-4251-819b-5084a847605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b26e7f-567e-4254-bb85-73fce2fdb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Space: 496 GB\n",
      "Used Space:  73 GB\n",
      "Free Space:  422 GB\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Check space where the container is running (usually '/')\n",
    "total, used, free = shutil.disk_usage(\"/\") # use /nvme and not /\n",
    "\n",
    "print(f\"Total Space: {total // (2**30)} GB\")\n",
    "print(f\"Used Space:  {used // (2**30)} GB\")\n",
    "print(f\"Free Space:  {free // (2**30)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e316d52-cf46-45a4-b1d7-ec9af84fd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Audio\n",
    "\n",
    "ds = load_dataset(\"kaarthu2003/SlrCvVoicesTtsDataset\")\n",
    "train_dataset = ds[\"train\"]\n",
    "val_dataset = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f532dc75-fe25-4c09-a55a-34cc98b4c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 15811\n",
      "Validation size: 1610\n",
      "\n",
      "Sample example:\n",
      "{'audio': <datasets.features._torchcodec.AudioDecoder object at 0x7427eaa2ec30>, 'sentence': 'దాగుడుమూతల ఆట వల్ల'}\n"
     ]
    }
   ],
   "source": [
    "# Print confirmation\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "# Sample peek\n",
    "print(\"\\nSample example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c426e8fb-db42-41f2-b53c-6cd98aec3b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ఇంకా తగ్గడానికి లేదా</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>మీకు ఉంటుందా అండి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ఇవి తినడం వల్ల రక్తపోటు తగ్గుతుంది</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>హలో మ్యామ్ చెప్పండి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ఒకటి ఏమో క్రోసిన్</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ఇప్పుడు చేసి ఇవ్వమంటారా</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>సార్ చెప్పండి ఎవరు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ఈరోజు ఈయన మరణ వార్త ఈనాడు లో వచ్చింది</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>థ్యాంక్యూ సార్ థ్యాంక్యూ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>వారికి పొత్తికడుపు వద్ద కలసి యుంది</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(train_dataset.remove_columns([\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4c8ff0-df1c-44a9-b2c1-16f26b18b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_special_unwanted_characters = [\n",
    "    'ఁ',  # Chandrabindu\n",
    "    'ౄ',  # Vocalic RR\n",
    "    'ౢ',  # Vocalic L\n",
    "    'ౣ',  # Vocalic LL\n",
    "    'ౠ',  # Long Vocalic RR\n",
    "    'ఽ',  # Avagraha\n",
    "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',  # Telugu digits\n",
    "    'ఀ',  # Telugu Sign Combining Candrabindu Above\n",
    "    'ౘ',  # Letter TTHA\n",
    "    'ౙ',  # Letter DDA\n",
    "    'ౚ',  # Letter RHA\n",
    "    '౷',  # Vedic Tone\n",
    "    '‘', '’', '“', '”', '%', '.', ';', '-', ',', '/', '\\\\', '_', '&',  # Common punctuation\n",
    "    'G', 'P', 'S', 'e', 'l', 'n', 'r', 't', '\\u200c', '\\n' #Unwanted in the dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af5f049-e24d-4d02-a778-de3e3c624356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_remove_regex = f'[{re.escape(\"\".join(telugu_special_unwanted_characters))}]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d1e48b-21e3-4cc1-8d85-501e8b840692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(remove_special_characters)\n",
    "val_dataset = val_dataset.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53847015-43eb-4f6f-8da4-4c96e69187f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>సరే సరే అండీ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>బాగానే ఉంటుంది సార్</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>మొదటి స్థానంలో జైపూర్ జిల్లా ఉంది</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ఉడకని అన్నం తింటే కడుపు నొప్పి వస్తుంది</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>పళ్ళ సరుకులు ఇవ్వండి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>అట్లాంటివి ఏం రావనమాట</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>అడై ఇది తమిళ వంటకం</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>అలాగే వర్షాకాలం సమయాల్లో</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>కాని ఈ గ్రామానికి ఎటువంటి వసతులు లేవు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>హలో నమస్కారం అండి</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(train_dataset.remove_columns([\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dac2381-f2e6-4a76-9014-e089f5d3d40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806b0d5711e64d42bad21e8fd4fa8c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f2ac6db4844249a444974e1b031201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = train_dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_dataset.column_names)\n",
    "vocab_test = val_dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30198df-3f41-4a21-ba08-a53b914446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a61dd2-88b5-4f6d-b548-b720345d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8f83fe-407f-41f3-8ff2-be5e2b1d50ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4059123-5374-4722-9b0b-112e83e987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52940772-a6af-44d1-88db-50712fa7d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\", clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "396829ad-ef13-44a3-948f-c9bd5408b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"wav2vec2-IEEEAccess-FinalRun-4Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d46de588-8cf8-4329-970f-5eb14f6934ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets/commit/a0c443f9a14b88427c91c154d3e03a23b73f2d7b', commit_message='Upload tokenizer', commit_description='', oid='a0c443f9a14b88427c91c154d3e03a23b73f2d7b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets', endpoint='https://huggingface.co', repo_type='model', repo_id='kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd6534f-f094-4a72-aa1d-7710a828574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25c715ab-54aa-44d1-a1ff-d1c36e57fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24a4455a-cb95-4961-a75c-682770596687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.features._torchcodec.AudioDecoder at 0x74271d3acf80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aedc9f16-9438-4059-ae85-2aa89146cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bab9985-612b-43fc-a8f3-a8bf9b7ae0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: బావి నీటిలోన ప్రతిబింబమును చూపి\n",
      "Input array shape: (76608,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(train_dataset))\n",
    "\n",
    "print(\"Target text:\", train_dataset[rand_int][\"sentence\"])\n",
    "print(\"Input array shape:\", train_dataset[rand_int][\"audio\"][\"array\"].shape)\n",
    "print(\"Sampling rate:\", train_dataset[rand_int][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e4a1cae-84a2-482a-81c2-c9675316d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"sentence\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27320cdf-1786-4d3b-a1af-6dc763dd7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc = 4)\n",
    "val_dataset = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names, num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b259ed1-0118-4e14-af2b-ade6962fd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "235d9e6e-d02e-4fba-ac73-a49cc132cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42c1e805-b51d-4fd7-b89a-62b4f5206216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "373f4dde-3bb9-47b7-b35e-e0b62c8c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ca0c0c4-3108-4ffb-bf13-c7c8950bcae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    'facebook/wav2vec2-large-xlsr-53',\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b852e0f-1a9d-48e2-8029-09c6eefbe61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65849109-9cc7-48fa-a68f-76b9e493abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=45,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=1600,\n",
    "  eval_steps=1600,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_ratio=0.1,\n",
    "  save_total_limit=2,\n",
    "  report_to=\"wandb\",\n",
    "  push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "415ce502-9f54-4a90-acff-9cfa9ab0a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import RMSprop\n",
    "optimizer = RMSprop(model.parameters(), lr=3e-4, alpha=0.99, eps=1e-8)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=processor.feature_extractor,\n",
    "    optimizers=(optimizer, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a516f415-7c43-4fa7-b9c0-a55a38df7ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/offline-run-20260113_064644-kuutld93</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22275' max='22275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22275/22275 6:03:03, Epoch 45/45]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.137200</td>\n",
       "      <td>0.580700</td>\n",
       "      <td>0.609557</td>\n",
       "      <td>0.154065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.788300</td>\n",
       "      <td>0.437679</td>\n",
       "      <td>0.418914</td>\n",
       "      <td>0.099903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.395227</td>\n",
       "      <td>0.377775</td>\n",
       "      <td>0.089746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>0.409413</td>\n",
       "      <td>0.358460</td>\n",
       "      <td>0.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.479748</td>\n",
       "      <td>0.390568</td>\n",
       "      <td>0.088844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.493569</td>\n",
       "      <td>0.350809</td>\n",
       "      <td>0.085199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.487408</td>\n",
       "      <td>0.319453</td>\n",
       "      <td>0.077466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.208100</td>\n",
       "      <td>0.511010</td>\n",
       "      <td>0.322714</td>\n",
       "      <td>0.078917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.552120</td>\n",
       "      <td>0.318073</td>\n",
       "      <td>0.077041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.521337</td>\n",
       "      <td>0.300263</td>\n",
       "      <td>0.074281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.540050</td>\n",
       "      <td>0.291860</td>\n",
       "      <td>0.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.560380</td>\n",
       "      <td>0.282328</td>\n",
       "      <td>0.069309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.571724</td>\n",
       "      <td>0.276935</td>\n",
       "      <td>0.067646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22275, training_loss=0.5167214729488899, metrics={'train_runtime': 21809.5961, 'train_samples_per_second': 32.623, 'train_steps_per_second': 1.021, 'total_flos': 6.590037917914548e+19, 'train_loss': 0.5167214729488899, 'epoch': 45.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b489c2eb-c679-45f2-a3ac-a0bdcd84d461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0e7bb5d8074fc78a6099a5f9947caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cc2b983c3747ebbf066493ff3b9cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets/commit/2f9daf4d11490e1b6df2231170e4db5a5444ef13', commit_message='End of training', commit_description='', oid='2f9daf4d11490e1b6df2231170e4db5a5444ef13', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets', endpoint='https://huggingface.co', repo_type='model', repo_id='kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8ec1623-512d-4276-93ce-9e4e24a2a7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a979d1bec8436ea0ba04c686230b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7a0a3b5af242d39088e09d7b4fa695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06defeaf204544f18f9a5beab1fd6396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf30a2f2fa96463099197799337bd8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/886 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50748afcd0c84c24b2991cb29644f11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbd011ebaa54957bb71fc318e1fbbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c5c78ae4ff49898024a09ba6a87b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets\")\n",
    "model = AutoModelForCTC.from_pretrained(\"kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35840f44-fc52-48b6-8d7a-1bab7afe498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function map_to_result at 0x74262feafba0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e9e67bc6594d4d937ccd18ffaf6068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "    # Now the model and input_values are on the same device\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "  return batch\n",
    "\n",
    "results = val_dataset.map(map_to_result, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26ab1de9-7f0d-49d4-bc1b-0d78ada4b41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.273\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d60b93ab-208c-4b9e-8e00-fac79df6f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "019a70c4-4a3b-42b6-8efe-0565f5a063be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CER: 0.068\n"
     ]
    }
   ],
   "source": [
    "print(\"Test CER: {:.3f}\".format(cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bfb11-e449-456c-b0ab-25492a970736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
