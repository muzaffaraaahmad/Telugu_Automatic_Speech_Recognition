{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a2b046-13c7-42a5-b365-8757f612954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available!\n",
      "Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ GPU is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ GPU is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2063e3-8fae-4b0f-9cef-90ba7625f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install librosa\n",
    "!pip install jiwer\n",
    "!pip install evaluate\n",
    "!pip install wandb\n",
    "!pip install numpy==1.23.5\n",
    "!pip install scipy==1.11.4\n",
    "!pip install librosa==0.10.1\n",
    "!pip install numba==0.58.1\n",
    "!pip install datasets>=2.14.0\n",
    "!pip install accelerate>=0.26.0\n",
    "!pip install typing_extensions --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1232436-f697-416b-9088-3017691d4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb1e23d-e6b4-423d-88a0-09c7c0424ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub --quiet\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb4912f-7663-4251-819b-5084a847605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b26e7f-567e-4254-bb85-73fce2fdb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Space: 548 GB\n",
      "Used Space:  93 GB\n",
      "Free Space:  427 GB\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Check space where the container is running (usually '/')\n",
    "total, used, free = shutil.disk_usage(\"/\") # use /nvme and not /\n",
    "\n",
    "print(f\"Total Space: {total // (2**30)} GB\")\n",
    "print(f\"Used Space:  {used // (2**30)} GB\")\n",
    "print(f\"Free Space:  {free // (2**30)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e316d52-cf46-45a4-b1d7-ec9af84fd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Audio\n",
    "\n",
    "ds = load_dataset(\"kaarthu2003/SlrCvVoicesTtsDataset\")\n",
    "train_dataset = ds[\"train\"]\n",
    "val_dataset = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f532dc75-fe25-4c09-a55a-34cc98b4c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 15811\n",
      "Validation size: 1610\n",
      "\n",
      "Sample example:\n",
      "{'audio': <datasets.features._torchcodec.AudioDecoder object at 0x7980e375ec60>, 'sentence': 'దాగుడుమూతల ఆట వల్ల'}\n"
     ]
    }
   ],
   "source": [
    "# Print confirmation\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "# Sample peek\n",
    "print(\"\\nSample example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c426e8fb-db42-41f2-b53c-6cd98aec3b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>చాలామంది ఆటల కంటే</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>విజయవాడ బస్ స్టాండ్</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>పాఠాలు వినలేడు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>చింతపండు కాల్ కిలో</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>దీనిలో ఏడు కాండాలున్నాయి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>వాళ్ళ ఇంటి దగ్గరకి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>పెద్ద పెద్ద బండలను తొలిచి ధ్వజస్తంభం ఆకృతులుగా మలుస్తారు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>తెల్లగా ఉన్నాయి మేడం</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>చిన్న పద్యమప్ప జెప్పలేడు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ఓకే ఓకే చెప్పండి</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(train_dataset.remove_columns([\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4c8ff0-df1c-44a9-b2c1-16f26b18b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_special_unwanted_characters = [\n",
    "    'ఁ',  # Chandrabindu\n",
    "    'ౄ',  # Vocalic RR\n",
    "    'ౢ',  # Vocalic L\n",
    "    'ౣ',  # Vocalic LL\n",
    "    'ౠ',  # Long Vocalic RR\n",
    "    'ఽ',  # Avagraha\n",
    "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',  # Telugu digits\n",
    "    'ఀ',  # Telugu Sign Combining Candrabindu Above\n",
    "    'ౘ',  # Letter TTHA\n",
    "    'ౙ',  # Letter DDA\n",
    "    'ౚ',  # Letter RHA\n",
    "    '౷',  # Vedic Tone\n",
    "    '‘', '’', '“', '”', '%', '.', ';', '-', ',', '/', '\\\\', '_', '&',  # Common punctuation\n",
    "    'G', 'P', 'S', 'e', 'l', 'n', 'r', 't', '\\u200c', '\\n' #Unwanted in the dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af5f049-e24d-4d02-a778-de3e3c624356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_remove_regex = f'[{re.escape(\"\".join(telugu_special_unwanted_characters))}]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d1e48b-21e3-4cc1-8d85-501e8b840692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(remove_special_characters)\n",
    "val_dataset = val_dataset.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53847015-43eb-4f6f-8da4-4c96e69187f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>పాటగాడికి పెద్ద సమస్యే పట్టుకున్నది కొయ్యకత్తితో మనిషితల ఎలా నరకటం</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>కలము పట్టగానె కవిశేఖరుడు గాడు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>త్యాంక్ యూ మ్యామ్</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ప్రిన్సిపాల్ ఒక మొమెంటోతో ప్రవేశించారు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>పరిసరాలను శుభ్రంగా ఉంచుచున్నారు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>సమాజానికి వివిధ రకాలుగా</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>హలో నమస్తే మ్యాడమ్</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ఫెసిలిటీస్ ఏమేమి ఉంటాయండి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>మంచిర్యాల జోగులాంబ గద్వాల్</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>వర్గ వివరణ పేజీలో క్రింది వివరణ ఉండాలి</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(train_dataset.remove_columns([\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dac2381-f2e6-4a76-9014-e089f5d3d40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac0f73226d64238b6b9f46ac78d0cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ab4bd6da2940c49765cb12ce59fe82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = train_dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_dataset.column_names)\n",
    "vocab_test = val_dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30198df-3f41-4a21-ba08-a53b914446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a61dd2-88b5-4f6d-b548-b720345d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8f83fe-407f-41f3-8ff2-be5e2b1d50ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4059123-5374-4722-9b0b-112e83e987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52940772-a6af-44d1-88db-50712fa7d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\", clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "396829ad-ef13-44a3-948f-c9bd5408b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"wav2vec2-IEEEAccess-FinalRun-4Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d46de588-8cf8-4329-970f-5eb14f6934ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets/commit/d1c103c560214000c1c0fa67f60d0c9d847abb8b', commit_message='Upload tokenizer', commit_description='', oid='d1c103c560214000c1c0fa67f60d0c9d847abb8b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets', endpoint='https://huggingface.co', repo_type='model', repo_id='kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd6534f-f094-4a72-aa1d-7710a828574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25c715ab-54aa-44d1-a1ff-d1c36e57fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24a4455a-cb95-4961-a75c-682770596687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.features._torchcodec.AudioDecoder at 0x79808c2ee5a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aedc9f16-9438-4059-ae85-2aa89146cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bab9985-612b-43fc-a8f3-a8bf9b7ae0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: మరికొద్దిసేపటికల్లా కోతులరూపంలో కవలసోదరులు తిరుగుతున్న చోటికి గెడ్డపువాడు వచ్చి ప్రత్యక్షమయ్యాడు\n",
      "Input array shape: (155689,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(train_dataset))\n",
    "\n",
    "print(\"Target text:\", train_dataset[rand_int][\"sentence\"])\n",
    "print(\"Input array shape:\", train_dataset[rand_int][\"audio\"][\"array\"].shape)\n",
    "print(\"Sampling rate:\", train_dataset[rand_int][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e4a1cae-84a2-482a-81c2-c9675316d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"sentence\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27320cdf-1786-4d3b-a1af-6dc763dd7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc = 4)\n",
    "val_dataset = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names, num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b259ed1-0118-4e14-af2b-ade6962fd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "235d9e6e-d02e-4fba-ac73-a49cc132cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42c1e805-b51d-4fd7-b89a-62b4f5206216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "373f4dde-3bb9-47b7-b35e-e0b62c8c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ca0c0c4-3108-4ffb-bf13-c7c8950bcae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    'facebook/wav2vec2-large-xlsr-53',\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b852e0f-1a9d-48e2-8029-09c6eefbe61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65849109-9cc7-48fa-a68f-76b9e493abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=20,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=1600,\n",
    "  eval_steps=1600,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_ratio=0.1,\n",
    "  save_total_limit=2,\n",
    "  report_to=\"wandb\",\n",
    "  push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "415ce502-9f54-4a90-acff-9cfa9ab0a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import RMSprop\n",
    "optimizer = RMSprop(model.parameters(), lr=3e-4, alpha=0.99, eps=1e-8)\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=processor.feature_extractor,\n",
    "    optimizers=(optimizer, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a516f415-7c43-4fa7-b9c0-a55a38df7ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/offline-run-20260104_132718-mnfp5bgw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9900' max='9900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9900/9900 2:07:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.467869</td>\n",
       "      <td>0.498056</td>\n",
       "      <td>0.118765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.422302</td>\n",
       "      <td>0.385551</td>\n",
       "      <td>0.092312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.403441</td>\n",
       "      <td>0.369873</td>\n",
       "      <td>0.085128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.339800</td>\n",
       "      <td>0.422895</td>\n",
       "      <td>0.335758</td>\n",
       "      <td>0.078917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.496672</td>\n",
       "      <td>0.328860</td>\n",
       "      <td>0.077148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.511337</td>\n",
       "      <td>0.314311</td>\n",
       "      <td>0.074122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9900, training_loss=0.7719459988372495, metrics={'train_runtime': 7700.0757, 'train_samples_per_second': 41.067, 'train_steps_per_second': 1.286, 'total_flos': 2.928353847999346e+19, 'train_loss': 0.7719459988372495, 'epoch': 20.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b489c2eb-c679-45f2-a3ac-a0bdcd84d461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aab71f86f94358b55f2f09153adeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cc4776952040c296d7d594bd0e55cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets/commit/0c3bf74845fb51fab886972953d30a7cebd21419', commit_message='End of training', commit_description='', oid='0c3bf74845fb51fab886972953d30a7cebd21419', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets', endpoint='https://huggingface.co', repo_type='model', repo_id='kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8ec1623-512d-4276-93ce-9e4e24a2a7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92a92dc5e404b94ac66615eba30e351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8fc2cdf8ec4848b67cf6009b83248d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0147b673a13d44c3a61bfb200148eaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9600750bbc44a2f93103d88774d1df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/886 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d285bcece9a4cc9aaa3b31d0216df6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93c99eded4745fc9db722a175ac73a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f895fce8a4a89af7c005a27f48cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets\")\n",
    "model = AutoModelForCTC.from_pretrained(\"kaarthu2003/wav2vec2-IEEEAccess-FinalRun-4Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35840f44-fc52-48b6-8d7a-1bab7afe498d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f849dccf5202484aa404b32d9754bff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "    # Now the model and input_values are on the same device\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "  return batch\n",
    "\n",
    "results = val_dataset.map(map_to_result, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26ab1de9-7f0d-49d4-bc1b-0d78ada4b41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.310\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d60b93ab-208c-4b9e-8e00-fac79df6f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "019a70c4-4a3b-42b6-8efe-0565f5a063be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CER: 0.074\n"
     ]
    }
   ],
   "source": [
    "print(\"Test CER: {:.3f}\".format(cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
